MLP: An MLP is a feedforward neural network that maps the lagged inputs (sliding-window features) to the next-step target via one or more hidden layers with nonlinear activations. With sufficient hidden units, MLPs approximate complex nonlinear dynamics. Important hyperparameters include the hidden size (`size`), regularization (`decay`), and the input window length.

Objective: Demonstrate how to train, validate, and evaluate an MLP (Multilayer Perceptron) model for time-series forecasting with sliding windows, including data preparation, normalization, model fitting, and evaluation with metrics and plots.

```{r}
# Time Series Regression - MLP

# Installing the package (if needed)
#install.packages("tspredit")
```

```{r}
# Loading the packages
library(daltoolbox)
library(tspredit) 
```


```{r}
# Series for study (generates sliding windows t9..t0)

data(tsd)
ts <- ts_data(tsd$y, 10)
ts_head(ts, 3)
```

```{r}
# Original series visualization
library(ggplot2)
plot_ts(x=tsd$x, y=tsd$y) + theme(text = element_text(size=16))
```

```{r}
# Train-test split and projection (X, y)

samp <- ts_sample(ts, test_size = 5)
io_train <- ts_projection(samp$train)
io_test <- ts_projection(samp$test)
```

```{r}
# Preprocessing (global min-max normalization)

preproc <- ts_norm_gminmax()
```

```{r}
# Training the MLP model

model <- ts_mlp(ts_norm_gminmax(), input_size=4, size=4, decay=0)
model <- fit(model, x=io_train$input, y=io_train$output)
```

```{r}
# Fit evaluation (train)

adjust <- predict(model, io_train$input)
adjust <- as.vector(adjust)
output <- as.vector(io_train$output)
ev_adjust <- evaluate(model, output, adjust)
ev_adjust$mse
```

```{r}
# Forecast on test set (5 steps ahead)

prediction <- predict(model, x=io_test$input[1,], steps_ahead=5)
prediction <- as.vector(prediction)
output <- as.vector(io_test$output)
ev_test <- evaluate(model, output, prediction)
ev_test
```

```{r}
# Plot comparing actual vs fit (train) and forecast (test)

yvalues <- c(io_train$output, io_test$output)
plot_ts_pred(y=yvalues, yadj=adjust, ypre=prediction) + theme(text = element_text(size=16))
```

References
- D. E. Rumelhart, G. E. Hinton, and R. J. Williams (1986). Learning representations by back-propagating errors. Nature, 323, 533â€“536.
